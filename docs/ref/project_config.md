# Project Configuration
Whistler's configuration file serves two purposes. First, it provides the fundamentals to the application that are required to perform the task at hand such basic study details, paths to source data, their associated data-dictionaries, the paths to the respective harmony files, etc, endpoint associations with the various development stages: local, dev, qa and prod. Second, it provides a record of the components associated with an ingest for interrogation at a later date. 

The file is a simple [YAML](https://yaml.org/) file and is required for all whistler command line tools. Because the user must provide the filename with every command, there is no firm requirement as to how one might name their configuration. In fact, for related datasets, one might even have multiple configuration files present in the same directory, using the same whistle projections. That said, to inform potential readers as to the format of the file, convention suggests that the file should have the extension, "yaml". 

The root level of the YAML file contains a combination of Study specific variables, project specific information such as the whistle entrypoint and project directory, development environment aliases, curies and, finally, the dataset list. 

## Whistler Application Variables
### output
The directory where all output from the execution (except harmony JSON files) are written. This includes the input to whistle as well as the final projected data produced by the whistle code. 

### id_colname
(tbd)

### projector_lib
The directory where all whistle source resides. 

### whistle_src
This is the *entrypoint* for the whistle execution. It *must* exist outside the project_lib directory. In general, it will be a very simple file containing only a single line instructing whistle to run a top level function defined inside the projector library. An example of what file might look like is shown below:
```
$this: Transform_Dataset($root);
```
For this example to work, Transform_Dataset must be defined in the projector library and must accept the root contents of the whistle input JSON file generated by play. 

### code_harmonization_dir
This is the path to the directory where the Harmony CSV resides. Unlike the rest of the output from Whistler, the resulting Harmony JSON file containing the actual ConceptMap will be written to this directory, which doesn't have to actually be inside the directory specified by the output parameter above. 

### curies
This is a list of Ontology URLs and their curies. This is used during the Harmony ConceptMap creation allowing for codes that are not prefixed by their curies be transformed in the Harmony maps target code. This is completely optional. If your ontologies do not require curies or the curies are consistently a part of the source data itself, then this is not required. 

An example of a curie configuration chunk can be seen below:
```
curies:
  http://purl.obolibrary.org/obo/hp.owl: HP
  http://purl.obolibrary.org/obo/mondo.owl: MONDO
  http://purl.obolibrary.org/obo/maxo.owl: MAXO
```

### env
This provides a shorthand for the common development environments and their mappings to an entry in the *fhir_hosts* file. 

An example of an env configuration chunk can be seen below:
```
env:
  local: dev
  dev: dev-fhir-svr
  qa: qa-fhir-svr
  prod: prod-fhir-svr
```
In this example, the end user can use the argument -e dev to specify to play or delfhir to use the host associated with dev-fhir-svr in the *fhir_hosts* file. This is just a convenience for cases where the hosts file contains information for servers that aren't specific to the current study or where each study has different target endpoints but a common projection library. 

## The Dataset List, dataset
Each data table must be added as an entry to the configuration property, dataset. Each *dataset* must have the following properties:

### filename
This points directly to the source data associated with this particular data table. For situations where the data is actually split into multiple files, the value assigned to filename can be a comma separated list of filenames. 

For datasets that have multiple files, all files should conform to the same data-dictionary. 

### data_dictionary (required)
This entry is an object with the following properties:
#### filename 
This should point to the data-dictionary file associated with this data table.  
#### colnames
An optional object containing alternate spellings for the data dictionary column names expected by whistler. Each property's key is the expected column name, and the value is the name found in the actual data-dictionary. 

An example of what a data-dictionary entry might look like is shown below:
```
    filename: data/tables/BRI-DSR/specimen.csv
    data_dictionary:
      filename: data/dd/specimen-dd.csv
      colnames:
        varname: attribute
        desc: description
        type: type
        values: valid values
```

### embed
For tables that are actually components of rows from another table, ETL authors can *embed* a table inside another. During the transformation into JSON, the target table's entries will contain an additional variable where the *embedded* rows will be found.

When the join criteria for an embedded row requires more than one column, all column names must be specified as part of the same value separated by ",". Authors referencing columns that must be cleaned prior to reaching whistle can use either original variable named formatted as the source data used, or the cleaned version as will be seen by whistle. Variable cleaning is just replacing whitespace with "_" and dropping variable names to all lower case. 

Consider the following example:
```
  file_manifest:
    filename: data/tables/BRI-DSR/file_meta_data.csv
    embed:
      dataset: specimen
      colname: sample_id
    data_dictionary:
      filename: data/dd/file_meta_data-dd.csv
```
Rows from the data table, file_meta_data.csv, will be *embedded* into rows from the specimen table based on matching *sample_id* values. As an example, a specimen whose sample_id matched 4 entries in the file_meta_data.csv file would have an extra variable called *file_meta_data* which would contain 4 objects, each representing one of the 4 rows from the file_meta_data table. 

This is much faster than letting whistle scan a list of file_meta_data objects searching for matching sample IDs. 

#### group_by
When data should be aggregated together with common values for one or more variables, the ETL author can specify this behavior in the configuration using the group_by property for that dataset entry. This results in a single object with distinct values for those keys with an addition property, *content* which holds each of the objects representing rows with a common set of values for those group_by columns. 

The field name for the the group by statement can be a single variable name or more than one separated by commands. Authors referencing columns that must be cleaned prior to reaching whistle can use either original variable named formatted as the source data used, or the cleaned version as will be seen by whistle. Variable cleaning is just replacing whitespace with "_" and dropping variable names to all lower case. 

An example of a dataset which uses the group_by parameter:
```
  aliquot:
    filename: data/csv/biospecimen/aliquot.csv
    subject_id: participantid
    group_by: Sample ID
    key_columns: Barcode
    data_dictionary:
      filename: data/csv/biospecimen/aliquot-dd.csv
    code_harmonization: harmony/htp-harmony.csv
```

This may result in rows that look like this:
```
"aliquot": [
  {
    "sample_id": "SAMPLE001",
    "content": [
      {
        "barcode": "001234",
        "vial_volumn": "0",
        "volume_unit": "ml"
      }
      {
        "barcode": "0124012",
        "vial_volume": "1",
        "volume_unit": "ml"
      }
    ]
  }
]
```

### subject_id
(TBD)

### key_columns
The key_columns field is used to indicate which column(s) express uniqueness across a data table if it isn't the column name defined by the *id_colname* property in the configuration. For compound keys, multiple column names (cleaned or not) should be separated by commas.

## Study Specific Components
When building the input for the Whistle call, a "Study" object is created and is populated by the values from the following configuration fields. This study object is available at the input JSON's root level and can be passed on to any of the whistle project functions at the ETL author's discretion. 

### study_id (required)
The study id the primary shortname for a study. This will be the string used in the meta.tag, so it shouldn't be too long and should be unique for a given destination server. 

### study_accession
This variable can be used to annotate a DbGAP study accession code. 

### study_title
This should be the formal title associated with the study at hand

### study_desc
Descriptive text providing a more in-depth overview of the study's purpose which can ultimately be provided as part of the study's FHIR resource. 

### url
This should be the study's official URL with regard to the destination FHIR server.

### identifier_prefix (required)
This variable is used by Whistler when generating URLs for CodeSystems and Resource Identifier. 

### require_official
This is intended to facilitate a request by the INCLUDE portal devs that key identifiers be marked as official. 